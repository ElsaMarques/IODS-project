---
title: "Chapter4"
author: "Elsa Marques"
date: "11/22/2017"
output: html_document
---

# Clustering and classification

*This chapter will teach you how one can cluster and classify datasets. There is different methods to perform the clustering of the data, but in a very simple point of view you cluster what is similar. For that you can use different parameters, from simple concepts of means to more complex concepts like data populations distances.*

## *About this week working data set "Boston"* 
  > The data set Boston is included in the library {MASS} and refers to the "Housing Values in Suburbs of Boston". The data compiled in the dataset orginates from the following papers: [Harrison and Rubinfeld (1978)](https://www.law.berkeley.edu/files/Hedonic.PDF) & [Belsley et al 1980](http://onlinelibrary.wiley.com/book/10.1002/0471725153)
 
## **The data to be used in this exercise will be loaded from the library {MASS}.** 

```{r, echo=FALSE}
# access the MASS package and other library that will be used in this exercise latter on
library(MASS)
library(dplyr)
library(tidyverse)
library(GGally)

# load the data
data("Boston")
```

The Boston data set includes the Housing Values in Suburbs of Boston (from 1978 - 1980). The information included in this data set includes values regarding Boston city crime, nitrogen oxide concentrations, average rooms per dwelling, lower status of the population, pupil-teacher ratio in town, etc.. 

```{r, echo=FALSE}
#explore the data set  
str(Boston)
dim(Boston)
```

You can see that the data includes a total of 506 observations distributed into the following 14 variables:
  - crim = per capita crime rate by town.
  - zn = proportion of residential land zoned for lots over 25,000 sq.ft.
  - indus = proportion of non-retail business acres per town.
  - chas = Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
  - nox = nitrogen oxides concentration (parts per 10 million).
  - rm = average number of rooms per dwelling.
  - age = proportion of owner-occupied units built prior to 1940.
  - dis = weighted mean of distances to five Boston employment centres.
  - rad = index of accessibility to radial highways.
  - tax = full-value property-tax rate per \$10,000.
  - ptratio = pupil-teacher ratio by town.
  - black = 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
  - lstat = lower status of the population (percent).
  - medv = median value of owner-occupied homes in \$1000s.

## **Lets have a graphical overview of the data** 

```{r, echo=FALSE}
library(ggplot2) 
ggpairs(Boston, title = "Housing Values in Suburbs of Boston", upper = "blank", )
```

 - From the graphical overview using the ggpairs between the variables you can start by observing which of the variables are normally distributed and which of them presented skewed data. In this data set most of the variables presented themselves skewed in either direction, being the rm (average number of rooms per dwelling) the only variable that presents a normal distribution. 


### **For a better understanding of the data lets also have a look on the summaries of the variables included in this dataset.** 
```{r, echo=FALSE}
library(dplyr)
summary(Boston)
```

 - From the variables summary you can mainly check the ranges of the data in the different variables. The skweded distrubition of most variables is now noticeable when you compare the Mean values and the minimum and maximum values of the data. 
 
### **For a better understanding on how the variables relate to each other, lets visualize a correlation plot.** 
```{r, echo=FALSE}
library(corrplot) 

# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="square", type="upper", order = "hclust", tl.col="black", tl.srt=45)
```


 - By drawing the correlation plot you can see some variables are highly positive or negatively correlated with each other. 
 
 - High positive numbers (Blue) mean that variables value will shift together in the same direction. For example, when you look at the crime data, you can see its higly positvely correlated with the tax, rad and lstat variables. Meaning that the higher the index of acessibility to radial highways, the higher value of property-tax rate per $10.000, and the amount of low status of the population the bigger amount of crime you can witness on those areas. 
 
  - On the other side, you can also observe some High negative numbers (Red) meaning that while the value of one variable increases the other one will decrease. For example, you can see how lstat (percentage of the lower status of the population) inversely correlates with medv (median value of owner-occupied homes in \$1000s) and rm (average number of rooms per dwelling). Meaning that the higher amount of low status population the less amount of rooms per dwelling, and the less amount of owner occupied homes with the value of $1000.

 
## **Standardizing the dataset and explore the scaled data summary and how variables change.**
```{r, echo=FALSE}
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)
```

  - When scaling the data, you just change the scale from which the data distributes in the x axis, not changing the values of the data itself. One can think this process is like grabbing the axis at the two ends and stretching or compressing it in order to make all your variables vary in the same comparable range, making it easier to compare. The scaling takes in account the mean (average) and standard deviation of the data. 


### **Create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate). Quantiles from the old "crim"" variable will be used as the break points for the new "crime"" categorical variable.**
```{r, echo = FALSE}
# summary of the scaled crime rate
summary(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, label = c("low", "med_low", "med_high", "high"))

# look at the table of the new factor crime
table(crime)
```

 - The new crime variable is now easier to interpret the amount of data fiiting in the the new assigned labels: "low", "med_low", "med_high", "high". You can also verify how the distribution between this categories is quite even.    
 
### **Since the new crime category looks more informative, let's drop the old crim variable from the dataset.** 
```{r, echo=FALSE}
# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# check how boston_scaled looks like now
boston_scaled %>% group_by(crime)

```

  - Now you can easily explore how the other variables vary according to the high or low crime rate category, and to have a better understanding on that I group the variables by crime categories. 

### **Divide the dataset to train and test sets, so that 80% of the data belongs to the train set.**
```{r, echo=FALSE}
# number of rows in the Boston dataset 
n <- nrow(boston_scaled)
n

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]
train

# create test set 
test <- boston_scaled[-ind,]
test
```


## **Fit the linear discriminant analysis on the train set. Use the categorical crime rate ("crime") as the target variable and all the other variables in the dataset as predictor variables. Draw the LDA (bi)plot.**
```{r, echo=FALSE}
library(dplyr)
# linear discriminant analysis
lda.fit <- lda(crime ~ . , data = train)

# print the lda.fit object
lda.fit

# function for the lda biplot arrows
lda.arrows <- function(x, myscale = 2, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)
```

- According to the linear description fit you can see how most of the high crime category correlates with the rad (index of accessibility to radial highways).


## **Save the crime categories from the test set and then remove the categorical crime variable from the test dataset. Then predict the classes with the LDA model on the test data. Cross tabulate the results with the crime categories from the test set.**

```{r, echo = FALSE}
library(dplyr)
# save the correct classes from test data
correct_classes <- test$crime
correct_classes

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

  - You can see how the predicted and corrected data tend to agree on the most extreme values ("low" and "high"), in the "med-low" and "med-high" you can see the most discrepance between the correct data and the predicted one. 

## **Reload the Boston dataset and standardize the dataset (we did not do this in the Datacamp exercises, but you should scale the variables to get comparable distances). Calculate the distances between the observations.** 

```{r, echo=FALSE}
# load MASS and Boston
library(MASS)
data("Boston")

# standardize Boston dataset
boston_scaled <- scale(Boston)

# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(boston_scaled, method ="manhattan")

# look at the summary of the distances
summary(dist_man)
```

  - You can see how the two methods to calculate the distances between the variables , gives you as result different numbers, but you can also note how the trend between the variations is maintained. 
  
### **Run k-means algorithm on the dataset. Investigate what is the optimal number of clusters and run the algorithm again. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors):**

```{r, echo=FALSE}
# MASS, ggplot2 and Boston dataset are available
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
```

 - While checking for the optimal number of clusters you should look in the graph when defining the k max = 10 for the k value where the curve varies the most dramatically. That appears to be around k = 2
 
 - When exploring the pairs using the colors to separate the km clusters, you can see that the variable that better separetes the two clusters (Red Vs Black cluster) would be tax (full-value property-tax rate per \$10,000) and rad (index of accessibility to radial highways). An almost complete cluster separation can also be observed when you check the crim (per capita crime rate by town) and black (1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town) variables. 
 
## **Optional / Bonus sections:**  
###  **Perform k-means on the original Boston data with some reasonable number of clusters (> 2). Remember to standardize the dataset.**  

```{r, echo = FALSE}

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <-kmeans(boston_scaled, centers = 3)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
```

 -  For the reasonable amount of clusters (> 2) I select k = 3  as it was the following  position in the curve you could observe a dramatically change. 
 
 - Now when you check at the pairs with the clusters divided by colors, since we have more clusters it gets a bit more confusing to read the data. Nonetheless, after careful observation you the variables were you can more easily identify the different clusters (Red, Black and Green) are medv (median value of owner-occupied homes in \$1000s) and partialy successful will be age (proportion of owner-occupied units built prior to 1940).  
 
### **Then perform LDA using the clusters as target classes. Include all the variables in the Boston data in the LDA model. Visualize the results with a biplot (include arrows representing the relationships of the original variables to the LDA solution).**

I was trying the following approach but I guess I missed something, further work/learning required in this section ; if the peer review knows how to fix this and explain me what is wrong I highly appreciate it!! 

- k-means clustering
km <-kmeans(boston_scaled, centers = 3)

- linear discriminant analysis
lda.fit <- lda(km ~ . , data = boston_scaled$km)

- print the lda.fit object
lda.fit

- function for the lda biplot arrows
lda.arrows <- function(x, myscale = 2, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

- target classes as numeric
classes <- as.numeric(boston_scaled$km)

- plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 5)

- I got stuck in this section as unfortunetely I couldn't manage to fix the errors in my code

 
## **Super-Bonus: Run the provided code for the (scaled) train data that you used to fit the LDA. The code creates a matrix product, which is a projection of the data points.**
```{r, echo=FALSE}
model_predictors <- dplyr::select(train, -crime)

# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)

# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

# Create a 3D plot of the columns of the matrix product
library(ggplot2)
library(plotly)
plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', col  = train$crime)

```


### **Draw another 3D plot where the color is defined by the clusters of the k-means. How do the plots differ?**

```{r, echo=FALSE}

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', col = train$km)
```

-  If I performed this correcly it seems that they don't differ much. Maybe the clusters don't separate that well the the data in the extreme negative x values (-2 < x < -4 ).





